{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text classification with a bag-of-words linear classifier and pre-trained embeddings</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/pytorch/tutorials/blob/master/beginner_source/text_sentiment_ngrams_tutorial.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.840B.300d.zip: 2.18GB [37:39, 963kB/s]                                 \n",
      "100%|█████████▉| 2195219/2196017 [05:20<00:00, 8121.32it/s]"
     ]
    }
   ],
   "source": [
    "emb_vecs = torchtext.vocab.GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-20e047c82790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTEXT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Passed vocabulary is not of type Vocab",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b8047273e5af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n\u001b[0;32m----> 6\u001b[0;31m     root='./.data', ngrams=NGRAMS, vocab=emb_vecs)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/meili0/amiceli/pytorch-elli-py3.6/lib/python3.6/site-packages/torchtext/datasets/text_classification.py\u001b[0m in \u001b[0;36mAG_NEWS\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_setup_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AG_NEWS\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/meili0/amiceli/pytorch-elli-py3.6/lib/python3.6/site-packages/torchtext/datasets/text_classification.py\u001b[0m in \u001b[0;36m_setup_datasets\u001b[0;34m(dataset_name, root, ngrams, vocab, include_unk)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Passed vocabulary is not of type Vocab\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vocab has {} entries'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating training data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Passed vocabulary is not of type Vocab"
     ]
    }
   ],
   "source": [
    "NGRAMS = 1\n",
    "import os\n",
    "if not os.path.isdir('./.data'):\n",
    "\tos.mkdir('./.data')\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./.data', ngrams=NGRAMS, vocab=emb_vecs)\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0lines [00:00, ?lines/s]\u001b[A\n",
      "2373lines [00:00, 23728.78lines/s]\u001b[A\n",
      "4709lines [00:00, 23615.82lines/s]\u001b[A\n",
      "6860lines [00:00, 22940.68lines/s]\u001b[A\n",
      "9190lines [00:00, 23045.26lines/s]\u001b[A\n",
      "11287lines [00:00, 22378.21lines/s]\u001b[A\n",
      "13541lines [00:00, 22426.23lines/s]\u001b[A\n",
      "15763lines [00:00, 22363.37lines/s]\u001b[A\n",
      "18032lines [00:00, 22459.10lines/s]\u001b[A\n",
      "20450lines [00:00, 22948.38lines/s]\u001b[A\n",
      "22654lines [00:01, 22472.03lines/s]\u001b[A\n",
      "24965lines [00:01, 22657.68lines/s]\u001b[A\n",
      "27188lines [00:01, 22090.74lines/s]\u001b[A\n",
      "29457lines [00:01, 22265.83lines/s]\u001b[A\n",
      "31803lines [00:01, 22608.58lines/s]\u001b[A\n",
      "34053lines [00:01, 20098.32lines/s]\u001b[A\n",
      "36346lines [00:01, 20869.06lines/s]\u001b[A\n",
      "38643lines [00:01, 21455.34lines/s]\u001b[A\n",
      "41061lines [00:01, 22204.54lines/s]\u001b[A\n",
      "43499lines [00:01, 22812.77lines/s]\u001b[A\n",
      "45805lines [00:02, 22328.13lines/s]\u001b[A\n",
      "48182lines [00:02, 22740.51lines/s]\u001b[A\n",
      "50676lines [00:02, 23357.41lines/s]\u001b[A\n",
      "53304lines [00:02, 24161.36lines/s]\u001b[A\n",
      "55738lines [00:02, 23566.42lines/s]\u001b[A\n",
      "58111lines [00:02, 23348.57lines/s]\u001b[A\n",
      "60627lines [00:02, 23862.69lines/s]\u001b[A\n",
      "63024lines [00:02, 23394.03lines/s]\u001b[A\n",
      "65515lines [00:02, 23798.38lines/s]\u001b[A\n",
      "67904lines [00:02, 23174.51lines/s]\u001b[A\n",
      "70231lines [00:03, 22947.41lines/s]\u001b[A\n",
      "72533lines [00:03, 22706.51lines/s]\u001b[A\n",
      "74811lines [00:03, 22727.54lines/s]\u001b[A\n",
      "77088lines [00:03, 21892.65lines/s]\u001b[A\n",
      "79704lines [00:03, 23017.69lines/s]\u001b[A\n",
      "82028lines [00:03, 23058.47lines/s]\u001b[A\n",
      "84525lines [00:03, 23600.05lines/s]\u001b[A\n",
      "87068lines [00:03, 24119.36lines/s]\u001b[A\n",
      "89493lines [00:03, 23791.02lines/s]\u001b[A\n",
      "91983lines [00:04, 24112.75lines/s]\u001b[A\n",
      "94403lines [00:04, 21254.79lines/s]\u001b[A\n",
      "96738lines [00:04, 21840.42lines/s]\u001b[A\n",
      "98974lines [00:04, 21589.09lines/s]\u001b[A\n",
      "101341lines [00:04, 21974.59lines/s]\u001b[A\n",
      "103644lines [00:04, 22279.99lines/s]\u001b[A\n",
      "105893lines [00:04, 19516.96lines/s]\u001b[A\n",
      "108190lines [00:04, 20437.25lines/s]\u001b[A\n",
      "110517lines [00:04, 21210.25lines/s]\u001b[A\n",
      "112973lines [00:05, 22112.45lines/s]\u001b[A\n",
      "115279lines [00:05, 22387.46lines/s]\u001b[A\n",
      "117665lines [00:05, 22794.59lines/s]\u001b[A\n",
      "120000lines [00:05, 22506.64lines/s]\u001b[A\n",
      "\n",
      "0lines [00:00, ?lines/s]\u001b[A\n",
      "317lines [00:00, 3167.23lines/s]\u001b[A\n",
      "987lines [00:00, 3761.70lines/s]\u001b[A\n",
      "2215lines [00:00, 4749.77lines/s]\u001b[A\n",
      "3447lines [00:00, 5822.75lines/s]\u001b[A\n",
      "4613lines [00:00, 6851.01lines/s]\u001b[A\n",
      "5898lines [00:00, 7966.13lines/s]\u001b[A\n",
      "7157lines [00:00, 8951.27lines/s]\u001b[A\n",
      "8407lines [00:00, 9783.27lines/s]\u001b[A\n",
      "9537lines [00:00, 9879.47lines/s]\u001b[A\n",
      "10767lines [00:01, 10499.27lines/s]\u001b[A\n",
      "12090lines [00:01, 11191.92lines/s]\u001b[A\n",
      "13282lines [00:01, 10214.22lines/s]\u001b[A\n",
      "14587lines [00:01, 10925.06lines/s]\u001b[A\n",
      "15820lines [00:01, 11310.70lines/s]\u001b[A\n",
      "17055lines [00:01, 11601.62lines/s]\u001b[A\n",
      "18250lines [00:01, 11414.54lines/s]\u001b[A\n",
      "19576lines [00:01, 11909.34lines/s]\u001b[A\n",
      "20790lines [00:01, 11759.84lines/s]\u001b[A\n",
      "22008lines [00:01, 11881.85lines/s]\u001b[A\n",
      "23208lines [00:02, 11737.80lines/s]\u001b[A\n",
      "24391lines [00:02, 11729.70lines/s]\u001b[A\n",
      "25662lines [00:02, 12006.56lines/s]\u001b[A\n",
      "26981lines [00:02, 12338.63lines/s]\u001b[A\n",
      "28222lines [00:02, 11991.56lines/s]\u001b[A\n",
      "29462lines [00:02, 12109.34lines/s]\u001b[A\n",
      "30720lines [00:02, 12246.66lines/s]\u001b[A\n",
      "31971lines [00:02, 12321.79lines/s]\u001b[A\n",
      "33230lines [00:02, 12396.44lines/s]\u001b[A\n",
      "34542lines [00:02, 12602.92lines/s]\u001b[A\n",
      "35805lines [00:03, 12489.57lines/s]\u001b[A\n",
      "37056lines [00:03, 10953.70lines/s]\u001b[A\n",
      "38343lines [00:03, 11465.66lines/s]\u001b[A\n",
      "39640lines [00:03, 11877.88lines/s]\u001b[A\n",
      "40854lines [00:03, 11767.35lines/s]\u001b[A\n",
      "42108lines [00:03, 11988.05lines/s]\u001b[A\n",
      "43361lines [00:03, 12143.78lines/s]\u001b[A\n",
      "44586lines [00:03, 12026.48lines/s]\u001b[A\n",
      "45854lines [00:03, 12213.66lines/s]\u001b[A\n",
      "47082lines [00:04, 11683.56lines/s]\u001b[A\n",
      "48312lines [00:04, 11860.29lines/s]\u001b[A\n",
      "49543lines [00:04, 11990.08lines/s]\u001b[A\n",
      "50748lines [00:04, 11930.83lines/s]\u001b[A\n",
      "52066lines [00:04, 12277.57lines/s]\u001b[A\n",
      "53299lines [00:04, 12063.55lines/s]\u001b[A\n",
      "54604lines [00:04, 12341.93lines/s]\u001b[A\n",
      "55843lines [00:05, 6751.25lines/s] \u001b[A\n",
      "57043lines [00:05, 7770.64lines/s]\u001b[A\n",
      "58401lines [00:05, 8913.87lines/s]\u001b[A\n",
      "59547lines [00:05, 9548.22lines/s]\u001b[A\n",
      "60830lines [00:05, 10340.54lines/s]\u001b[A\n",
      "62081lines [00:05, 10907.92lines/s]\u001b[A\n",
      "63283lines [00:05, 10904.81lines/s]\u001b[A\n",
      "64462lines [00:05, 11155.55lines/s]\u001b[A\n",
      "65634lines [00:05, 11219.32lines/s]\u001b[A\n",
      "66816lines [00:05, 11392.90lines/s]\u001b[A\n",
      "68009lines [00:06, 11545.46lines/s]\u001b[A\n",
      "69200lines [00:06, 11650.57lines/s]\u001b[A\n",
      "70406lines [00:06, 11770.44lines/s]\u001b[A\n",
      "71594lines [00:06, 11442.84lines/s]\u001b[A\n",
      "72820lines [00:06, 11675.02lines/s]\u001b[A\n",
      "74009lines [00:06, 11737.37lines/s]\u001b[A\n",
      "75233lines [00:06, 11881.59lines/s]\u001b[A\n",
      "76426lines [00:06, 11617.03lines/s]\u001b[A\n",
      "77632lines [00:06, 11745.25lines/s]\u001b[A\n",
      "78810lines [00:06, 11735.43lines/s]\u001b[A\n",
      "80023lines [00:07, 11849.38lines/s]\u001b[A\n",
      "81359lines [00:07, 12264.51lines/s]\u001b[A\n",
      "82591lines [00:07, 12151.73lines/s]\u001b[A\n",
      "83826lines [00:07, 12210.25lines/s]\u001b[A\n",
      "85050lines [00:07, 11909.23lines/s]\u001b[A\n",
      "86245lines [00:07, 11749.39lines/s]\u001b[A\n",
      "87423lines [00:07, 11751.74lines/s]\u001b[A\n",
      "88665lines [00:07, 11943.66lines/s]\u001b[A\n",
      "89862lines [00:07, 11604.40lines/s]\u001b[A\n",
      "91027lines [00:08, 11592.77lines/s]\u001b[A\n",
      "92201lines [00:08, 11635.06lines/s]\u001b[A\n",
      "93382lines [00:08, 11686.96lines/s]\u001b[A\n",
      "94635lines [00:08, 11925.22lines/s]\u001b[A\n",
      "95830lines [00:08, 11786.35lines/s]\u001b[A\n",
      "97011lines [00:08, 11683.91lines/s]\u001b[A\n",
      "98199lines [00:08, 11740.13lines/s]\u001b[A\n",
      "99375lines [00:08, 11655.87lines/s]\u001b[A\n",
      "100620lines [00:08, 11883.01lines/s]\u001b[A\n",
      "101811lines [00:08, 11825.35lines/s]\u001b[A\n",
      "103051lines [00:09, 11978.10lines/s]\u001b[A\n",
      "104251lines [00:09, 11431.85lines/s]\u001b[A\n",
      "105514lines [00:09, 11766.23lines/s]\u001b[A\n",
      "106818lines [00:09, 12120.96lines/s]\u001b[A\n",
      "108038lines [00:09, 11756.96lines/s]\u001b[A\n",
      "109222lines [00:09, 11756.39lines/s]\u001b[A\n",
      "110403lines [00:09, 11635.05lines/s]\u001b[A\n",
      "111627lines [00:09, 11809.31lines/s]\u001b[A\n",
      "112812lines [00:09, 11790.71lines/s]\u001b[A\n",
      "113994lines [00:09, 11662.07lines/s]\u001b[A\n",
      "115220lines [00:10, 11832.60lines/s]\u001b[A\n",
      "116406lines [00:10, 11706.56lines/s]\u001b[A\n",
      "117615lines [00:10, 11818.51lines/s]\u001b[A\n",
      "120000lines [00:10, 11466.60lines/s]\u001b[A\n",
      "\n",
      "0lines [00:00, ?lines/s]\u001b[A\n",
      "788lines [00:00, 7877.28lines/s]\u001b[A\n",
      "1927lines [00:00, 8678.64lines/s]\u001b[A\n",
      "2434lines [00:00, 3156.00lines/s]\u001b[A\n",
      "3629lines [00:00, 4049.80lines/s]\u001b[A\n",
      "4964lines [00:00, 5119.78lines/s]\u001b[A\n",
      "6239lines [00:00, 6239.47lines/s]\u001b[A\n",
      "7600lines [00:01, 7413.05lines/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "NGRAMS = 1\n",
    "import os\n",
    "if not os.path.isdir('./.data'):\n",
    "\tos.mkdir('./.data')\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, masks, lens):\n",
    "        embedded = self.embedding(text)\n",
    "        # Avg bag-of-words\n",
    "        masked_emb = embedded * masks.unsqueeze(-1)\n",
    "        avg_emb = torch.sum(masked_emb, 1) / lens.unsqueeze(-1)\n",
    "        return self.fc(avg_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Generate the pad id\n",
    "pad_id = train_dataset.get_vocab()['<pad>']\n",
    "\n",
    "def pad_data(data):\n",
    "    # Find max length of the mini-batch\n",
    "    lens = list(zip(*data))[0]\n",
    "    max_len = max(lens)\n",
    "    label_list = list(zip(*data))[2]\n",
    "    txt_list = list(zip(*data))[3]\n",
    "    padded_tensors = torch.stack([torch.cat((txt, \\\n",
    "            torch.tensor([pad_id] * (max_len - len(txt))).long())) \\\n",
    "            for txt in txt_list])\n",
    "    lens = torch.LongTensor(lens)\n",
    "    mask = torch.arange(max_len)[None, :] < lens[:, None]\n",
    "    return padded_tensors, torch.LongTensor(label_list), mask, lens\n",
    "\n",
    "def get_data_loader(dataset_, **kwargs):\n",
    "    # Generate a list of tuples of text length, index, label, text\n",
    "    dataset_sorted = [(len(txt), idx, label, txt) for idx, (label, txt) in enumerate(dataset_)]\n",
    "    dataset_sorted.sort() # sort by length and pad sequences with similar lengths\n",
    "    return DataLoader(dataset_sorted, collate_fn=pad_data, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = get_data_loader(sub_train_, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for i, (text, labels, masks, lens) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, labels, masks, lens = (v.to(device) for v in (text, labels, masks, lens))\n",
    "        output = model(text, masks, lens)\n",
    "        loss = criterion(output, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == labels).sum().item()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = get_data_loader(data_, batch_size=BATCH_SIZE)\n",
    "    for text, labels, masks, lens in data:\n",
    "        text, labels, masks, lens = (v.to(device) for v in (text, labels, masks, lens))\n",
    "        with torch.no_grad():\n",
    "            output = model(text, masks, lens)\n",
    "            loss = criterion(output, labels)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == labels).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 22 seconds\n",
      "\tLoss: 0.0330(train)\t|\tAcc: 82.3%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 90.3%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 22 seconds\n",
      "\tLoss: 0.0163(train)\t|\tAcc: 91.4%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 91.4%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 22 seconds\n",
      "\tLoss: 0.0131(train)\t|\tAcc: 93.1%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 91.8%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 21 seconds\n",
      "\tLoss: 0.0111(train)\t|\tAcc: 94.2%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 92.2%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 21 seconds\n",
      "\tLoss: 0.0096(train)\t|\tAcc: 94.9%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 92.2%(valid)\n",
      "Epoch: 6  | time in 0 minutes, 22 seconds\n",
      "\tLoss: 0.0085(train)\t|\tAcc: 95.5%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 92.2%(valid)\n",
      "Epoch: 7  | time in 0 minutes, 22 seconds\n",
      "\tLoss: 0.0076(train)\t|\tAcc: 96.0%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 92.0%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 7\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0001(test)\t|\tAcc: 91.4%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        text_len = text.size()[0]\n",
    "        text = text.unsqueeze(0)\n",
    "        mask = torch.BoolTensor([[True]*text_len])\n",
    "        len_tensor = torch.LongTensor([text_len])\n",
    "        output = model(text, mask, len_tensor)\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
